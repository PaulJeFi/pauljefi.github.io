\documentclass[a4paper]{article}
\usepackage[a4paper, total={16cm, 22cm}]{geometry}
\usepackage[french]{babel}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{stmaryrd}

\newtheorem*{theorem}{Théorème}
\newtheorem*{remark}{Remarque}
\newtheorem*{example}{exemple}
\newtheorem*{proposition}{Proposition}
\newtheorem*{cor}{Corollaire}
\newtheorem*{definition}{Définition}

\title{Diagonalisation des matrices carrées réelles de rang 1}
\date{\today}
\author{Paul JÉRÔME-\--FILIO}

\begin{document}
\maketitle

Soit $n \in \mathbb{N}^*$. On s'intéresse à la diagonalisabilité et diagonalisation des matrices de $\mathcal{M}_{n}(\mathbb{R})$. Dans un premier temps, on cherche à montrer le théorème suivant :

\begin{theorem}
    Une matrice $A \in \mathcal{M}_{n}(\mathbb{R})$ est de rang $1$ si et seulement s'il existe $X, Y \in \mathcal{M}_{n, 1}(\mathbb{R})$ tels que $A = XY^\top$.
\end{theorem}

\begin{proof}
    $\boxed{\Rightarrow}$ Soit $A \in \mathcal{M}_{n}(\mathbb{R})$ une matrice de rang $1$. On note $c_1, c_2, \dots, c_n$ ses colonnes. Si $A$ est la matrice nulle, le sens direct du théorème est évidemment vrai. On suppose donc que $A$ n'est pas la matrice nulle. Il existe alors $i \in \llbracket 1, n \rrbracket$ telle que $c_i$ n'est pas la colonne nulle. Il existe donc $\left(\lambda_k\right)_{k \in \llbracket 1, n \rrbracket} \in \mathbb{R}^{\llbracket 1, n \rrbracket}$ tel que pour tout $k \in \llbracket 1, n \rrbracket$, $c_k = \lambda_k c_i$. On a alors :
    
    \begin{align*}
        A &= [c_1, c_2, \dots , c_n]\\
        &= [\lambda_1 c_i, \lambda_2 c_i, \dots , \lambda_n c_i]\\
        &= c_i \begin{pmatrix}\lambda_1 & \lambda_2 & \dots & \lambda_n \end{pmatrix}\\
        &= \begin{pmatrix}c_{i1} \\ c_{i2} \\ \vdots \\ c_{in} \end{pmatrix} \begin{pmatrix}\lambda_1 & \lambda_2 & \dots & \lambda_n \end{pmatrix}\\
        &= \begin{pmatrix}c_{i1} \\ c_{i2} \\ \vdots \\ c_{in} \end{pmatrix} \begin{pmatrix}\lambda_1 \\ \lambda_2 \\ \vdots \\ \lambda_n \end{pmatrix}^\top\\
        &= XY^\top \qquad \text{en posant}   \quad X = \begin{pmatrix}c_{i1} \\ c_{i2} \\ \vdots \\ c_{in} \end{pmatrix} \quad \text{et} \quad Y = \begin{pmatrix}\lambda_1 \\ \lambda_2 \\ \vdots \\ \lambda_n \end{pmatrix}
    \end{align*}
    
    $\boxed{\Leftarrow}$ Réciproquement, il suffit de remonter le raisonnement ci-dessus pour montrer que si \\ $X, Y \in \mathcal{M}_{n, 1}(\mathbb{R})$, alors $\operatorname{rg}\left(XY^\top\right)=1$.
    
\end{proof}

Bien que ce théorème soit inutile (ou du moins pas le plus rapide) pour traiter notre problème, il est intéressant car permet de montrer avec les théorèmes suivants que le produit scalaire de $\mathcal{M}_{n, 1}(\mathbb{R})$ défini par, pour tout $X, Y \in \mathcal{M}_{n, 1}(\mathbb{R})$, $(X|Y) = \operatorname{Tr}\left(XY^\top\right)$, renvoie l'unique valeur propre non-nulle de $XY^\top$ (le cas $n=1$ est évident).
On s'intéresse réellement au théorème suivant :

\begin{theorem}
    Si $n \neq 1$ et que la matrice $A \in \mathcal{M}_{n}(\mathbb{R})$ est de rang $1$, alors $0$ est une valeur propre de $A$ et $\dim(E_0(A)) = n-1$.
\end{theorem}

\begin{proof}
    Par le théorème du rang, on a $\dim(\ker(A)) = n - \operatorname{rg}(A) = n-1$.
    Pour que $0$ soit une valeur propre de $A$, il faut qu'il existe $X \in \mathcal{M}_{n,1}(\mathbb{R})$ non-nul tel que $AX = 0X = 0$, autrement dit $X$ doit être un vecteur non-nul de $\ker(A)$. Or comme $\ker(A)$ est de dimension $n-1 > 0$, alors il existe bien un tel $X$ non-nul. $0$ est bien une valeur propre de $A$ et le sous-espace propre associé vérifie $\dim(E_0(A)) = \dim(\ker(A-0 I_n)) = \dim(\ker(A)) = n-1$.
\end{proof}

\begin{remark}
    Pour montrer que $0$ est une valeur propre de $A$, on pouvait aussi procéder comme suit : comme $\operatorname{rg}(A)=1$, on a $\det(A) = 0$. Par ailleurs, on a $\displaystyle \det(A) = \prod_{\mu \in \operatorname{Sp}(A)} \mu^{m_\mu(A)}$. On tire donc \\$\displaystyle \prod_{\mu \in \operatorname{Sp}(A)} \mu^{m_\mu(A)} = 0$, donc par la règle du produit nul, $0 \in \operatorname{Sp}(A)$.
\end{remark}

On en déduit alors la proposition ci-après :

\begin{proposition}
    Si $n \neq 1$ et que la matrice $A \in \mathcal{M}_{n}(\mathbb{R})$ est de rang $1$, alors $A$ est diagonalisable si et seulement si $m_0(A) = n-1$. Dans ce cas, $\lambda = \operatorname{Tr}(A)$ est l'autre valeur propre de $A$, avec $m_\lambda(A) = 1$.
\end{proposition}

\begin{proof}
    $A$ est diagonalisable si et seulement si pour tout $\mu \in \operatorname{Sp}(A)$, $\dim(E_\mu(A)) = m_\mu(A)$. Il faut donc, d'après ce qui précède, $m_0(A) = \dim(E_0(A)) = n-1$ et une autre valeur propre $\lambda \neq 0$ avec $m_\lambda(A) = 1$. On a de plus $\displaystyle \operatorname{Tr}(A) = \sum_{\mu \in \operatorname{Sp}(A)} \mu \times m_\mu(A) = 1\times \lambda + 0(n-1) = \lambda$.
\end{proof}

On remarque que si $n=1$, alors $A \in \mathcal{M}_{n}(\mathbb{R})$ est déjà diagonale, et son unique valeur propre est son seul coefficient. Une conséquence immédiate de la proposition précédente est le théorème suivant :

\begin{theorem}
    Si $n \neq 1$ et que la matrice $A \in \mathcal{M}_{n}(\mathbb{R})$ est de rang $1$, alors $A$ est diagonalisable si et seulement si $\operatorname{Tr}(A) \neq 0$.
\end{theorem}

\begin{proof}
    En effet, $A$ est diagonalisable si et seulement si $A$ admet une autre (et unique) valeur propre $\lambda \neq 0$ et dans ce cas uniquement on a $\operatorname{Tr}(A) = \lambda \neq 0$. 
\end{proof}

On est alors en mesure de diagonaliser de tête des matrices de rang $1$.

\begin{example}
    Diagonaliser $M = \begin{pmatrix} 1 & 1 & 1 \\ 1 & 1 & 1 \\ 1 & 1 & 1 \end{pmatrix}$.
    
    $M$ est de rang $1$, et diagonalisable d'après le théorème spectral car symétrique réelle (pour la diagonalisabilité, on peut simplement remarquer que $\operatorname{Tr}(A) = 3 \neq 0$). D'après ce qui précède, on sait que $0$ est valeur propre de $M$ d'ordre de multiplicité $2$ et que son unique autre valeur propre est $\operatorname{Tr}(M) = 3$. Ainsi, $M$ est semblable à la matrice diagonale $D = \begin{pmatrix} 3 & 0 & 0 \\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{pmatrix}$.
    
    Un autre exemple est la matrice $\begin{pmatrix}1 & -1 \\ 1 & -1\end{pmatrix}$ : elle est de rang $1$ mais pas diagonalisable puisque sa trace est nulle.
\end{example}

La diagonalisation permettant simplement de déterminer les puissances successives d'une matrice, on peut s'intéresser aux puissances successives de notre matrice $A$ de rang $1$.

\begin{proposition}
    Si $A \in \mathcal{M}_{n}(\mathbb{R})$ est de rang $1$ et diagonalisable, alors $A^2 = \operatorname{Tr}(A)A$.
\end{proposition}

\begin{proof}
    D'après ce qui précède, $A$ peut se diagonaliser sous la forme $A = PDP^{-1}$ avec\\ $P \in \mathcal{G}\ell_n(\mathbb{R})$ et $D = \begin{pmatrix} \operatorname{Tr}(A) & 0 & \dots & 0 \\ 0 & 0 & \dots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \dots & 0\end{pmatrix} = \operatorname{Tr}(A) U$ en posant $U = \begin{pmatrix} 1 & 0 & \dots & 0 \\ 0 & 0 & \dots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \dots & 0\end{pmatrix}$. On a $U^2 = U$, puis 
    \begin{align*}
        A^2 &= A \times A = PDP^{-1}PDP^{-1} = PD^2P^{-1} \\
        &= P\left(\operatorname{Tr}(A)U\right)^2P^{-1} = P\operatorname{Tr}(A)^2U^2P^{-1} \\
        &= \operatorname{Tr}(A)P\operatorname{Tr}(A)UP^{-1} =  \operatorname{Tr}(A)PDP^{-1} \\
        &= \operatorname{Tr}(A)A
    \end{align*}
\end{proof}

On peut généraliser ce résultat au cas où $A$ n'est pas diagonalisable :

\begin{proposition}
    Si $A \in \mathcal{M}_{n}(\mathbb{R})$ est de rang $1$, alors $A^2 = \operatorname{Tr}(A)A$.
\end{proposition}

\begin{proof}
    Comme $A$ n'est \emph{a priori} pas diagonalisable, on ne peut pas passer par la matrice $U$ qui était bien pratique. On utilise le premier théorème : $A$ étant de rang 1, il existe $X, Y \in \mathcal{M}_{n, 1}(\mathbb{R})$ tels que $A = XY^\top$. On a de plus $Y^\top X = \operatorname{Tr}(A)$ en assimilant une matrice $1\times1$ à son coefficient réel. Alors $A^2 = XY^\top XY^\top = X\operatorname{Tr}(A)Y^\top = \operatorname{Tr}(A)XY^\top = \operatorname{Tr}(A)A$.
\end{proof}

\begin{remark}
    Si l'on ne voulait pas utiliser le premier théorème dans la démonstration, il reste à traiter le cas $\operatorname{Tr}(A) = 0$. On continue avec la réduction matricielle. Dans ce cas, $0$ est la seule valeur propre, d'ordre de multiplicité $n$ avec $\dim(E_0(A))=n-1$. On trigonalise donc $A$ sous la forme $A = PTP^{-1}$ avec $P \in \mathcal{G}\ell_n(\mathbb{N})$ et $T$ une matrice triangulaire supérieure sous la forme de Jordan :
    $$ T = \begin{pmatrix}
        0 & \dots & 0 & 0 & 0 \\
        \vdots  & \ddots & \vdots & \vdots & \vdots \\
        0 & \dots & 0 & 0 & 0 \\
        0 & \dots & 0 & 0 & 1 \\
        0 & \dots & 0 & 0 & 0 \\
    \end{pmatrix}$$
    
    On vérifie facilement par calcul matriciel que $T^2 = 0_{\mathcal{M}_{n}(\mathbb{R})}$, d'où finalement : $$A^2 = PTP^{-1} = PTP^{-1}PTP^{-1} = PT^2P^{-1} = 0_{\mathcal{M}_{n}(\mathbb{R})} = \operatorname{Tr}(A)A$$
\end{remark}

De ce résultat, on déduit le théorème suivant :

\begin{theorem}
    Si $A \in \mathcal{M}_{n}(\mathbb{R})$ est de rang $1$, alors pour tout $k \in \mathbb{N}^*$, on a $A^k = \operatorname{Tr}(A)^{k-1}A$.
\end{theorem}

\begin{proof}
    Par une récurrence immédiate.
\end{proof}

\begin{cor}
    Si $A \in \mathcal{M}_{n}(\mathbb{R})$ est de rang $1$, alors $A$ est nilpotente si et seulement si $\operatorname{Tr}(A) = 0$. Dans ce cas, $A$ est nilpotente d'ordre $2$.
\end{cor}

\begin{proof}
    L'énoncé est immédiat avec le théorème précédent.
\end{proof}

\begin{example}
    En reprenant les exemples précédents, $\begin{pmatrix} 1 & 1 & 1 \\ 1 & 1 & 1 \\ 1 & 1 & 1 \end{pmatrix} ^n = 3^{n-1} \begin{pmatrix} 1 & 1 & 1 \\ 1 & 1 & 1 \\ 1 & 1 & 1 \end{pmatrix}$ et pour tout $n > 1$, $\begin{pmatrix}1 & -1 \\ 1 & -1\end{pmatrix}^n = 0_{\mathcal{M}_{n}(\mathbb{R})}$.
\end{example}

On peut maintenant s'intéresser à l'exponentiation de matrices de rang $1$.

\begin{definition}
    On définit l'exponentielle d'une matrice $M \in \mathcal{M}_{n}(\mathbb{R})$, notée $\exp(M) = e^M$, par la matrice image de $M$ par l'application
    \begin{align*}
    \exp : \mathcal{M}_{n}(\mathbb{R}) &\longrightarrow \mathcal{M}_{n}(\mathbb{R})\\
    M &\longmapsto \exp(M) = e^M = \sum_{k=0}^{+\infty} \frac{1}{k!}M^k
    \end{align*}
\end{definition}

Et dans le cas des matrices de rang $1$, on a le théorème suivant :

\begin{theorem}
    Soit $A \in \mathcal{M}_{n}(\mathbb{R})$ une matrice de rang $1$.
    \begin{itemize}
        \item Si $\operatorname{Tr}(A) = 0$, alors $e^A = I_n +A$.
        \item Si $\operatorname{Tr}(A) \neq 0$, alors il existe $P \in \mathcal{G}\ell_n(\mathbb{R})$ telle que $e^A = P\operatorname{diag}(e^{\operatorname{Tr}(A)}, 1, \dots, 1)P^{-1}$.
    \end{itemize}
\end{theorem}

\begin{proof}
    \begin{itemize}
        \item Si $\operatorname{Tr}(A) = 0$, alors comme $A^k = 0$ pour tout entier $k \geq 2$,\\ on a $\displaystyle e^A = \sum_{k=0}^{+\infty} \frac{1}{k!}A^k = A^0 + A + \sum_{k=2}^{+\infty} \frac{1}{k!}A^k = I_n + A$.
        \item Si Si $\operatorname{Tr}(A) \neq 0$, alors d'après ce qui précède, $A$ est diagonalisable sous la forme $A = P \operatorname{diag}(\operatorname{Tr}(A), 0, \dots, 0)P^{-1}$ avec $P \in \mathcal{G}\ell_n(\mathbb{R})$. On montre par une récurrence immédiate que pour tout $k \in \mathbb{N}$,\\ $A^k =  P \operatorname{diag}(\operatorname{Tr}(A), 0, \dots, 0)^kP^{-1} =  P \operatorname{diag}\left(\operatorname{Tr}(A)^k, 0, \dots, 0\right)P^{-1}$. On a alors :
        \begin{align*}
            e^A &= \sum_{k=0}^{+\infty} \frac{1}{k!}A^k\\
            &= \sum_{k=0}^{+\infty}  \frac{1}{k!} P \operatorname{diag}(\operatorname{Tr}(A), 0, \dots, 0)P^{-1}\\
            &= P \left(\sum_{k=0}^{+\infty}  \frac{1}{k!} \operatorname{diag}(\operatorname{Tr}(A), 0, \dots, 0)\right) P^{-1}\\
            &= P \operatorname{diag}\left(\sum_{k=0}^{+\infty}  \frac{1}{k!} \operatorname{Tr}(A)^k, \sum_{k=0}^{+\infty}  \frac{1}{k!}, \dots, \sum_{k=0}^{+\infty}  \frac{1}{k!}\right) P^{-1}\\
            &= P \operatorname{diag}\left(e^{\operatorname{Tr}(A)}, e^0, \dots, e^0\right) P^{-1}\\
            &= P\operatorname{diag}\left(e^{\operatorname{Tr}(A)}, 1, \dots, 1\right)P^{-1}
        \end{align*}
    \end{itemize}
\end{proof}

\begin{example}
    On reprend les exemples précédents.\\
    On a $\operatorname{Tr}\begin{pmatrix}1 & -1 \\ 1 & -1\end{pmatrix} = 0_{\mathcal{M}_{n}(\mathbb{R})}$ d'où $\exp \begin{pmatrix}1 & -1 \\ 1 & -1\end{pmatrix} = \begin{pmatrix}1 & 0 \\ 0 & 1\end{pmatrix} + \begin{pmatrix}1 & -1 \\ 1 & -1\end{pmatrix} = \begin{pmatrix}2 & -1 \\ 1 & 0\end{pmatrix}$.\\
    Pour $M = \begin{pmatrix} 1 & 1 & 1 \\ 1 & 1 & 1 \\ 1 & 1 & 1 \end{pmatrix}$ de trace non-nulle, on la diagonalise sous la forme $M = PDP^{-1}$ avec \\$P = \begin{pmatrix} 1 & 1 & 1 \\ 1 & -1 & 0 \\ 1 & 0 & -1 \end{pmatrix}$, $D = \begin{pmatrix} 3 & 0 & 0 \\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{pmatrix}$ et on calcule $\setlength{\delimitershortfall}{0pt} P^{-1} =  \begin{pmatrix} \frac{1}{3} & \frac{1}{3} & \frac{1}{3} \\[1ex] \frac{1}{3} & -\frac{2}{3} & \frac{1}{3} \\[1ex] \frac{1}{3} & \frac{1}{3} & -\frac{2}{3} \end{pmatrix}$.\\On a alors $e^M = \begin{pmatrix} 1 & 1 & 1 \\ 1 & -1 & 0 \\ 1 & 0 & -1 \end{pmatrix} \begin{pmatrix} e^3 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{pmatrix}\setlength{\delimitershortfall}{0pt} \begin{pmatrix} \frac{1}{3} & \frac{1}{3} & \frac{1}{3} \\[1ex] \frac{1}{3} & -\frac{2}{3} & \frac{1}{3} \\[1ex] \frac{1}{3} & \frac{1}{3} & -\frac{2}{3} \end{pmatrix}$, \\d'où finalement, $\exp\begin{pmatrix} 1 & 1 & 1 \\ 1 & 1 & 1 \\ 1 & 1 & 1 \end{pmatrix} = \setlength{\delimitershortfall}{0pt} \begin{pmatrix} \dfrac{3}{2} + \dfrac{e^3}{3} & \dfrac{e^3-1}{3} & \dfrac{e^3-1}{3} \\[2ex] \dfrac{e^3-1}{3} & \dfrac{2}{3} + \dfrac{e^3}{3} & \dfrac{e^3-1}{3} \\[2ex] \dfrac{e^3-1}{3} & \dfrac{e^3-1}{3} & \dfrac{2}{3} + \dfrac{e^3}{3} \end{pmatrix}$
    
\end{example}

\end{document}